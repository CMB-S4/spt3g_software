<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>mpi &#8212; SPT3G Software  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="_static/nature.css?v=279e0f84" />
    <script src="_static/documentation_options.js?v=5929fcd5"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="maps" href="moddoc_maps.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="moddoc_maps.html" title="maps"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">SPT3G Software  documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">mpi</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="mpi">
<h1>mpi<a class="headerlink" href="#mpi" title="Link to this heading">¶</a></h1>
<p>mpi contains several modules build to interface with HPC systems using the installed MPI library (through mpi4py). This comes in two pieces: parallelization of frame processing and accumulation.</p>
<section id="frame-io-parallelization">
<h2>Frame IO Parallelization<a class="headerlink" href="#frame-io-parallelization" title="Link to this heading">¶</a></h2>
<p>The MPIFileIO sub-module contains a few modules that will add frame-level parallelism to a G3Pipeline by reading files within an observation in parallel and (optionally, with MPIIODistributor or MPIFrameParallelizer) distributes the data frame-by-frame across any number of processes with an M:N IO:CPU node distribution.</p>
<p>This processing will make sure all metadata frames are seen by all processes in the same order with respect to both each other and all data frames and are thus strongly ordered across all processes. Data frames (scans, typically) are ordered on a node, but are not ordered between nodes and thus should be considered weakly ordered. For a frame sequence MN12345PQ67, where letters denote metadata frames (calibration, etc.) and numbers data frames, a possible ordering seen by the pipelines on two processing nodes would be MN134PQ7 and MN25PQ6, respectively. A consequence of these ordering rules is that you <em>must not use any modules that depend on frame ordering and continuity</em>. Examples of such would be modules that buffer multiple scans together to get longer-time-period data; by their nature, these only work for observation-level parallelism, rather than frame-level or file-level.</p>
<p>Parallelization in this sense can be achieved by replacement of G3Reader in a normal G3Pipeline with MPIIODistributor.</p>
</section>
<section id="frame-accumulation">
<h2>Frame Accumulation<a class="headerlink" href="#frame-accumulation" title="Link to this heading">¶</a></h2>
<p>At the end of a sequence of parallel pipelines, you in general want to join the data from all the processes in the communicator back together again. Typical cases are stitching processed scans back together again, for example to feed to a maximum-likelihood map-maker or some other algorithm that needs very large quantities of distributed data, or doing a parallel reduction, for example by coadding map frames produced by N parallel map-makers.</p>
<p>The first of these cases (restitching an observation processed on many parallel processes) is more complicated than the second, so this library provides a helper module (MPIAccumulator) that does it for you, placing the result in a member variable inspected after the pipelines end.</p>
<p>The second case is easier, but potentially common enough that a module should be added in future.</p>
</section>
<section id="interface-to-toast">
<h2>Interface to TOAST<a class="headerlink" href="#interface-to-toast" title="Link to this heading">¶</a></h2>
<p>There is an experimental module (TOASTFiller) that uses MPIAccumulator to fill frame contents after processing in many parallel pipelines into a TOAST TOD object in order to use TOAST’s set of parallel timestream algorithms and map-making. Basic functionality is present, but it should be considered in-development at present.</p>
</section>
<section id="modules-in-spt3g-mpi">
<h2>Modules in spt3g.mpi<a class="headerlink" href="#modules-in-spt3g-mpi" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p><a class="reference internal" href="#spt3g-mpi-mpiaccumulator-mpiaccumulator">spt3g.mpi.MPIAccumulator.MPIAccumulator</a></p></li>
<li><p><a class="reference internal" href="#spt3g-mpi-mpifileio-mpifilereader">spt3g.mpi.MPIFileIO.MPIFileReader</a></p></li>
<li><p><a class="reference internal" href="#spt3g-mpi-mpifileio-mpiframeparallelizer">spt3g.mpi.MPIFileIO.MPIFrameParallelizer</a></p></li>
<li><p><a class="reference internal" href="#spt3g-mpi-mpifileio-mpiiodistributor">spt3g.mpi.MPIFileIO.MPIIODistributor</a></p></li>
</ul>
<p id="spt3g-mpi-mpiaccumulator-mpiaccumulator"><strong>spt3g.mpi.MPIAccumulator.MPIAccumulator</strong></p>
<p>Accumulate data from many frames in memory, sharing metadata for all the
frames between nodes in an MPI communicator, potentially organized into
groups (for example by observation ID).</p>
<p>The result will be stored in the member variable ‘fullobs’ after
pipeline termination. ‘fullobs’ is a dictionary containing entries
for each data group (see the extractfunc parameter), which in turn
are lists of 3-element tuples with the following content:</p>
<ul class="simple">
<li><p>First element is the metadata for the frame (see extractfunc)</p></li>
<li><p>Second element is the node on which the full data exist</p></li>
<li><p>Third element is the data from the frame (see extractfunc)
or None if the data are on a remote process.</p></li>
</ul>
<dl>
<dt><em>Constructor:</em></dt><dd><p><code class="docutils literal notranslate"><span class="pre">MPIAccumulator(mpicomm,</span> <span class="pre">extractfunc=None,</span> <span class="pre">sorter=None,</span> <span class="pre">dataframes=[spt3g.core.G3FrameType.Scan,</span> <span class="pre">spt3g.core.G3FrameType.Timepoint])</span></code></p>
</dd>
<dt><em>Constructor:</em></dt><dd><p>Data will be shared between nodes participating in the communicator
mpicomm. The data stored are based on frames of the types in the
dataframes argument and are organized based on the results of
<code class="docutils literal notranslate"><span class="pre">extractfunc</span></code> and <code class="docutils literal notranslate"><span class="pre">sorter</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">extractfunc</span></code> is expected to return a three-element tuple:</p>
<ul class="simple">
<li><p>First element is a group ID for the data (for example, an observation
ID). This is used only for organization and can be anything that
Python can compare. By default, this is SourceName-ObservationID.</p></li>
<li><p>Second element is metadata. This information is shared between
all processes participating in the communicator at completion and
should include any information you need to assess whether the data
are interesting for future calculations (for example, the start time
of the frame). By default, frames are sorted by this value. None
by default.</p></li>
<li><p>Third element is the data. This stays node-local. It could be just
a copy of the frame (the default) or just the information (in any
format understandable to Python) from the frame you expect to use
later.</p></li>
</ul>
<p>Note that <code class="docutils literal notranslate"><span class="pre">extractfunc</span></code> is called on all frame types (including
calibration data) but <em>only the results if it returns non-None values</em>
are stored. Thus, it is the responsibility of <code class="docutils literal notranslate"><span class="pre">extractfunc</span></code> to either
cache any applicable calibration, etc. information and store it in the
return value for data frames if needed or explicitly return a value
for calibration data, if needed. The default <code class="docutils literal notranslate"><span class="pre">extractfunc</span></code> saves only
the frames in the “dataframes” argument.</p>
<p><code class="docutils literal notranslate"><span class="pre">sorter</span></code> is used, if defined, to sort the list of frame data in each
group. This is passed to the Python sorted() function as a ‘key’
argument and receives the three-element tuple stored for each set of
frame data (metadata, node, data). By default, does no sorting
(frames will still appear in a common order across nodes related to
the data distribution if unsorted).</p>
</dd>
</dl>
<p id="spt3g-mpi-mpifileio-mpifilereader"><strong>spt3g.mpi.MPIFileIO.MPIFileReader</strong></p>
<p>Do parallel I/O and processing across an MPI communicator. The style of
parallelism here is that each process in the communicator gets a file, reads
the file, and processes the file. Supports shared files, which are read
from the lead process and then broadcasted to the others before they read
their own.</p>
<p>The list of files is specified as a list of lists. For example,
[‘offline_calibration.g3’, [‘0000.g3’, ‘0002.g3’, ‘0003.g3’]]
will cause all nodes to process ‘offline_calibration.g3’ and then the
remaining files (in the parallel block denoted by the nested list)
will be read and processed by the remaining nodes in a distributed way.</p>
<dl class="simple">
<dt><em>Constructor:</em></dt><dd><p><code class="docutils literal notranslate"><span class="pre">MPIFileReader(mpicomm,</span> <span class="pre">files)</span></code></p>
</dd>
</dl>
<p id="spt3g-mpi-mpifileio-mpiframeparallelizer"><strong>spt3g.mpi.MPIFileIO.MPIFrameParallelizer</strong></p>
<p>Do parallel I/O and processing across an MPI communicator. The style of
parallelism here is that a set of IO processes read files from disk
and distribute frames round-robin-style across a worker communicator.
Frames will arrive in order on each process, with gaps between time
segments, but no guarantees are made about relative order between
processes. All CPU nodes receive all metadata frames.</p>
<p>Rules to make this work:
- First module on IO nodes must be MPIFileReader
- Second module on IO nodes must be DeduplicateMetadata
- Third module on IO nodes, first on workers, must be MPIFrameParallelizer</p>
<p>You may want to use the MPIIODistributor pipe segment to make sure these
rules are followed.</p>
<dl class="simple">
<dt><em>Constructor:</em></dt><dd><p><code class="docutils literal notranslate"><span class="pre">MPIFrameParallelizer(iocomm,</span> <span class="pre">cpucomm,</span> <span class="pre">cpucomm_startrank,</span> <span class="pre">dataframetype=[spt3g.core.G3FrameType.Timepoint,</span> <span class="pre">spt3g.core.G3FrameType.Scan])</span></code></p>
</dd>
<dt><em>Constructor:</em></dt><dd><p>Parellizes a frame stream across a communicator, distributing frames
from M IO processes that are reading them to N CPU processes that
are analyzing them. iocomm is a communicator containing exactly the
set of IO nodes. cpucomm is a communicator containing (at least)
all the IO and all the CPU nodes that the IO and CPU nodes can use
to communicate with each other. The set of CPU nodes is the set of
processes in cpucomm with rank greater than or equal to
cpucomm_startrank. All frame types other than those in dataframetype
appear on all CPU nodes, while frames of types in dataframetype
are processed only by a single (random) CPU node.</p>
</dd>
</dl>
<p id="spt3g-mpi-mpifileio-mpiiodistributor"><strong>spt3g.mpi.MPIFileIO.MPIIODistributor</strong></p>
<blockquote>
<div><p>Read files from disk using the first n_io processes in mpicomm (COMM_WORLD
by default), with processing of frames in those files occurring on the other
processes in mpicomm. See documentation for MPIFileReader for the format of
the files argument and MPIFrameParallelizer for information on the semantics
of processing. Add this as the first module in your pipeline in place of
core.G3Reader.</p>
</div></blockquote>
<p>Equivalent to:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ne">Exception</span> <span class="n">evaluating</span> <span class="n">equivalence</span> <span class="p">(</span><span class="n">No</span> <span class="n">module</span> <span class="n">named</span> <span class="s1">&#39;mpi4py&#39;</span><span class="p">)</span>
</pre></div>
</div>
<dl class="simple">
<dt><em>Definition:</em></dt><dd><p><code class="docutils literal notranslate"><span class="pre">MPIIODistributor(pipe,</span> <span class="pre">mpicomm=None,</span> <span class="pre">n_io=10,</span> <span class="pre">files=[])</span></code></p>
</dd>
</dl>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">mpi</a><ul>
<li><a class="reference internal" href="#frame-io-parallelization">Frame IO Parallelization</a></li>
<li><a class="reference internal" href="#frame-accumulation">Frame Accumulation</a></li>
<li><a class="reference internal" href="#interface-to-toast">Interface to TOAST</a></li>
<li><a class="reference internal" href="#modules-in-spt3g-mpi">Modules in spt3g.mpi</a></li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="moddoc_maps.html"
                          title="previous chapter">maps</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/moddoc_mpi.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="moddoc_maps.html" title="maps"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">SPT3G Software  documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">mpi</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; Copyright .
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.2.3.
    </div>
  </body>
</html>